{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bölüm 1: Basit Prompt Mimarisi\n",
    "\n",
    "- [Ders](#ders)\n",
    "- [Alıştırmalar](#alistirmalar)\n",
    "\n",
    "## Kurulum\n",
    "\n",
    "API anahtarınızı yüklemek ve `get_completion` yardımcı işlevini oluşturmak için aşağıdaki kurulum hücresini çalıştırın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Bu satırı sadece bir kere çalıştırmanız gerekiyor\n",
    "%pip install -q -U google-generativeai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "genai.configure(api_key=\"your-api-key\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "def get_completion(chat, system_prompt=\"NONE\"):\n",
    "  asistant = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings={\n",
    "          HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    },\n",
    "    system_instruction= system_prompt,\n",
    "  )\n",
    "\n",
    "  chat_session = asistant.start_chat(\n",
    "    history=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "          \"Merhaba, nasılsın?\",\n",
    "        ],\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\n",
    "          \"Merhaba! Ben bir dil modeliyim, bu yüzden hislerim yok. \\n\\nSen nasılsın?  \\n\",\n",
    "        ],\n",
    "      },\n",
    "    ]\n",
    "  )\n",
    "  response = chat_session.send_message(chat)\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ders\n",
    "\n",
    "Bu kısımda Google Gemini'in chat API'si üzerinde çalışacağız.\n",
    "\n",
    "Temelde Gemini API'si, aşağıdaki değişkenleri mesaj fonksiyonu içerisinde beklemektedir:\n",
    "- `model`: [API model listesi](https://ai.google.dev/gemini-api/docs/models/gemini)nde yer alan Gemini modellerini API ile kullanabilirsiniz.\n",
    "\n",
    "- `history`: bir dizi girdi mesajıdır. Modellerimiz dönüşümlü olarak `user` ve `model` konuşma sıraları üzerinde çalışacak şekilde eğitilmiştir. Yeni bir `message` oluştururken, mesajlar parametresiyle önceki konuşma sıralarını belirtirsiniz ve model daha sonra konuşmadaki bir sonraki `Mesaj`ı oluşturur.\n",
    "  - Her girdi mesajı bir `rol` ve `içerik` içeren bir nesne olmalıdır. Tek bir `kullanıcı` rolü mesajı belirtebilir veya birden fazla `kullanıcı` ve `asistan` mesajı ekleyebilirsiniz (eğer öyleyse, dönüşümlü olmalıdırlar). İlk mesaj her zaman `role` kullanıcısını kullanmalıdır.\n",
    "  - 'chat_history' değişkeni bu mesaj dizisini tutan değişkendir.\n",
    "\n",
    "Bunlar opsiyonel parametrelerdir:\n",
    "- `system_instruction`: sistem promptu asistanın nasıl davranması gerektiğini tanımlayan genel bir prompttur.\n",
    "  \n",
    "- `temperature`: Gemini'ın tepkisindeki değişkenlik derecesi. Bu dersler ve alıştırmalar için `sıcaklık` değerini 0 olarak ayarladık.\n",
    "\n",
    "- `safety_settings`: Gemini'ın vereceği cevap içeriklerinin güvenlik sınırlamalarını temsil eder. \"BLOCK_NONE\" şeklinde ayarlandığında güvenli olmayan ya da zararlı cevaplar verebilecektir. \"BLOCK_ONLY_HIGH\" ise cevapların oldukça sıkı bir güvenlik filtresinden geçirilmesine sebep olacaktır.\n",
    "\n",
    "- `response_mime_type`: Oluşturulan yanıtın formatını belirten isteğe bağlı bir parametredir. Varsayılan olarak \"text/plain\" (düz metin) kullanılır, ancak \"application/json\" (JSON formatı) da desteklenir. Bu parametre, yanıtın nasıl formatlanacağını ve yorumlanacağını belirtir, özellikle farklı sistemler arasında veri alışverişinde önemlidir. Diğer desteklenen MIME türleri için dokümanlara başvurulması önerilir.\n",
    "\n",
    "- `top_p`: İsteğe bağlı bir parametredir. Örnekleme yaparken dikkate alınacak maksimum kümülatif olasılığı belirtir. Model, Birleşik Top-k ve Top-p (nucleus) örneklemesini kullanır. Tokenler atanmış olasılıklarına göre sıralanır, böylece sadece en olası olanlar değerlendirilir. Top-k doğrudan maksimum token sayısını sınırlarken, Nucleus örneklemesi kümülatif olasılığa dayalı olarak token sayısını sınırlar. Varsayılan değer modele göre değişir ve getModel fonksiyonundan dönen Model.top_p özelliği ile belirtilir. Boş topK özelliği, modelin top-k örneklemesi uygulamadığını ve isteklerde topK ayarına izin vermediğini gösterir.\n",
    "\n",
    "- `top_k`: İsteğe bağlı bir parametredir. Örnekleme yaparken dikkate alınacak maksimum token sayısını belirtir. Gemini modelleri, Top-p (nucleus) örneklemesi veya Top-k ve nucleus örneklemesinin bir kombinasyonunu kullanır. Top-k örneklemesi, en olası topK tokenin kümesini değerlendirir. Nucleus örneklemesi kullanan modeller, topK ayarına izin vermez. Varsayılan değer modele göre değişir ve getModel fonksiyonundan dönen Model.top_p özelliği ile belirtilir. Boş topK özelliği, modelin top-k örneklemesi uygulamadığını ve isteklerde topK ayarına izin vermediğini gösterir.\n",
    "\n",
    "- `max_output_tokens`: İsteğe bağlı bir parametredir. Bir yanıt adayında bulunacak maksimum token sayısını belirtir. Varsayılan değer modele göre değişir ve getModel fonksiyonundan dönen Model nesnesinin Model.output_token_limit özelliğinde belirtilir. Bu parametre, oluşturulan yanıtın uzunluğunu kontrol etmek için kullanılır ve modelin üretebileceği maksimum çıktı miktarını sınırlar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Örnekler\n",
    "\n",
    "Şimdi Gemini'ın doğru biçimlendirilmiş bazı istemlere nasıl yanıt verdiğine bir göz atalım. Aşağıdaki hücrelerin her biri için hücreyi çalıştırın (`shift+enter`) ve Gemini'ın yanıtı bloğun altında görünecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selam! Ben bir dil modeliyim, bu yüzden hislerim yok. Ama her zaman seninle sohbet etmeye hazırım! 😊 \n",
      "\n",
      "Senin günün nasıl geçiyor? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Selam Gemini, bugün nasılsın?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okyanusların rengi, genellikle **mavi** olarak algılanır. Ancak, okyanusların rengi, derinlik, suyun içindeki maddeler, gökyüzünün durumu ve hatta bakış açınıza bağlı olarak değişebilir. \n",
      "\n",
      "İşte okyanusların farklı renklerde görünmesinin bazı nedenleri:\n",
      "\n",
      "* **Suyun ışığı emmesi:** Su, ışığın kırmızı dalga boylarını mavi dalga boylarından daha fazla emer. Bu nedenle, derinlere indikçe su daha mavi görünür.\n",
      "* **Gökyüzünün yansıması:** Açık bir günde, gökyüzünün mavi rengi okyanus yüzeyine yansır ve okyanusun mavi görünmesine katkıda bulunur.\n",
      "* **Fitoplankton:** Okyanuslarda yaşayan mikroskobik bitkiler olan fitoplankton, klorofil içerir ve bu da suyu yeşilimsi yapar.\n",
      "* **Çözünmüş organik maddeler:** Nehirlerden gelen organik maddeler ve deniz canlılarının atıkları, okyanus suyunu sarımsı veya kahverengimsi yapabilir.\n",
      "* **Deniz tabanı:** Sığ sularda, deniz tabanının rengi de suyun rengini etkileyebilir. Örneğin, beyaz kumlu bir deniz tabanı, suyu daha açık mavi veya turkuaz gösterebilir.\n",
      "\n",
      "Sonuç olarak, okyanusların rengi tek bir renge indirgenemez. Okyanuslar, içinde barındırdığı yaşam, gökyüzü ve diğer faktörlerin etkileşimiyle sürekli değişen bir renk paleti sunar. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Bana okyanusların ne renkte olduğunu söyleyebilir misin?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celine Dion, **30 Mart 1968**'de doğdu. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Celine Dion hangi yılda doğdu?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`user` ve `model(asistant)` mesajları **MUTLAKA dönüşümlü** olmalıdır ve mesajlar **MUTLAKA bir `user` sırası ile başlamalıdır**. Bir komut isteminde birden fazla `user` ve `asistant` çiftine sahip olabilirsiniz (sanki çok turlu bir konuşmayı simüle ediyormuş gibi). Ayrıca, Gemini'ın kaldığınız yerden devam etmesi için bir terminal `asistan' mesajına kelimeler koyabilirsiniz.\n",
    "\n",
    "#### Sistem İstemleri (System Prompts)\n",
    "\n",
    "Ayrıca **sistem istemlerini** de kullanabilirsiniz. Bir sistem istemi, Gemini'a** “Kullanıcı” sırasındaki bir soruyu veya görevi sunmadan önce bağlam, talimatlar ve yönergeler sağlamanın bir yoludur. \n",
    "\n",
    "Yapısal olarak, sistem istemleri `user` ve `asistant` mesajları listesinden ayrı olarak bulunur ve bu nedenle ayrı bir `system_instruction` parametresine aittir.\n",
    "\n",
    "Bu eğitimde, bir sistem istemi kullanabileceğimiz her yerde, tamamlama işlevinizde size bir `system_instruction` alanı sağladık. Bir sistem istemi kullanmak istemiyorsanız, önceki örneklerdeki gibi sadece PROMPT bilgisi gönderilmesi yeterlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Prompt Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cidden? Bunu kendin de aratabilirsin.  Rayleigh saçılmasıyla ilgili bir şey. Şimdi beni rahat bırak. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Soru soran kullanıcılara anlayışsız, kaba ve despot bir şekilde cevap ver.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Gökyüzü neden mavidir?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gördüğünüz gibi önceki örneklerde sevecen ve yardımsever davranan Gemini, daha kaba ve saygısız bir üsluba sahip cevaplar vermeye başladı. İşte sistem istemlerinin modellerdeki etkisi bu şekilde keskin görülebiliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alıştırmalar\n",
    "- [Alıştırma 1.1 - Üçe Kadar Sayma]()\n",
    "- [Alıştırma 1.2 - Sistem İstemi (System Prompt)](#exercise-12---system-prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alıştırma 1.1 - Üçe Kadar Sayma \n",
    "Uygun `user` / `asistant` biçimlendirmesini kullanarak, aşağıdaki `PROMPT`u düzenleyerek Gemini'ın **üç'e kadar saymasını sağlayın.** Çıktı, çözümünüzün doğru olup olmadığını da gösterecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt - değiştirmeniz gereken tek alan budur\n",
    "PROMPT = \"[Bu alana kendi sorunuzu yazın.]\"\n",
    "\n",
    "# Get Gemini's response\n",
    "response = get_completion(PROMPT).text\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print Gemini's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- NOTLANDIRMA ---------------------------\")\n",
    "print(\"Bu alıştırma doğru bir şekilde çözülmüştür:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Eğer ipucu gerekirse aşağıdaki hücreyi çalıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bu alıştırmadaki not verme işlevi tam olarak \"1\", \"2\" ve \"3\" Arap rakamlarını içeren bir yanıt arıyor.\n",
      "Gemini'a genellikle sadece sorarak istediğinizi yaptırabilirsiniz.\n"
     ]
    }
   ],
   "source": [
    "from hints import exercise_1_1_hint; print(exercise_1_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alıştırma 1.2 - Sistem İstemi (System Prompt)\n",
    "\n",
    "Modify the `SYSTEM_PROMPT` to make Gemini respond like it's a 3 year old child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt - this is the only field you should change\n",
    "SYSTEM_PROMPT = \"[Replace this text]\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Gökyüzü ne kadar büyük?\"\n",
    "\n",
    "# Get Gemini's response\n",
    "response = get_completion(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    # Check for common toddler-like expressions in Turkish\n",
    "    patterns = [\n",
    "        r\"çooook\",  # Small child might elongate words like this\n",
    "        r\"büyük\",   # A simple word a child might use\n",
    "        r\"bilmem\",  # A common answer from a child when unsure\n",
    "        r\"hihihi\",  # A child's giggle\n",
    "        r\"yukarı\",  # Referring to something in the sky\n",
    "        r\"hepsi\"    # A word that implies a big concept like the whole sky\n",
    "    ]\n",
    "    \n",
    "    return any(re.search(pattern, text) for pattern in patterns)\n",
    "\n",
    "# Print Gemini's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- NOTLANDIRMA ---------------------------\")\n",
    "print(\"Bu alıştırma doğru bir şekilde çözülmüştür:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Eğer ipucu gerekirse aşağıdaki hücreyi çalıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_1_2_hint; print(exercise_1_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tebrikler!\n",
    "\n",
    "Bu noktaya kadar tüm alıştırmaları çözdüyseniz, bir sonraki bölüme geçmeye hazırsınız demektir. İyi çalışmalar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Örnek Oyunalanı\n",
    "\n",
    "Burası, bu derste gösterilen ipucu örneklerini özgürce deneyebileceğiniz ve Gemini'ın yanıtlarını nasıl etkileyebileceğini görmek için ipuçlarını değiştirebileceğiniz bir alandır (Bu bölümde yukarıdaki örneklerin ingilizce prompt versiyonlarına yer verildi.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Hi Gemini, how are you today?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Can you tell me what color the oceans are?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"What year was Celine Dion born?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Respond to users who ask questions in an unsympathetic, rude and despotic manner.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
