{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BÃ¶lÃ¼m 1: Basit Prompt Mimarisi\n",
    "\n",
    "- [Ders](#ders)\n",
    "- [AlÄ±ÅŸtÄ±rmalar](#alistirmalar)\n",
    "\n",
    "## Kurulum\n",
    "\n",
    "API anahtarÄ±nÄ±zÄ± yÃ¼klemek ve `get_completion` yardÄ±mcÄ± iÅŸlevini oluÅŸturmak iÃ§in aÅŸaÄŸÄ±daki kurulum hÃ¼cresini Ã§alÄ±ÅŸtÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Bu satÄ±rÄ± sadece bir kere Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekiyor\n",
    "%pip install -q -U google-generativeai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "genai.configure(api_key=\"your-api-key\")\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "def get_completion(chat, system_prompt=\"NONE\"):\n",
    "  asistant = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    "    safety_settings={\n",
    "          HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "    },\n",
    "    system_instruction= system_prompt,\n",
    "  )\n",
    "\n",
    "  chat_session = asistant.start_chat(\n",
    "    history=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "          \"Merhaba, nasÄ±lsÄ±n?\",\n",
    "        ],\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\n",
    "          \"Merhaba! Ben bir dil modeliyim, bu yÃ¼zden hislerim yok. \\n\\nSen nasÄ±lsÄ±n?  \\n\",\n",
    "        ],\n",
    "      },\n",
    "    ]\n",
    "  )\n",
    "  response = chat_session.send_message(chat)\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ders\n",
    "\n",
    "Bu kÄ±sÄ±mda Google Gemini'in chat API'si Ã¼zerinde Ã§alÄ±ÅŸacaÄŸÄ±z.\n",
    "\n",
    "Temelde Gemini API'si, aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri mesaj fonksiyonu iÃ§erisinde beklemektedir:\n",
    "- `model`: [API model listesi](https://ai.google.dev/gemini-api/docs/models/gemini)nde yer alan Gemini modellerini API ile kullanabilirsiniz.\n",
    "\n",
    "- `history`: bir dizi girdi mesajÄ±dÄ±r. Modellerimiz dÃ¶nÃ¼ÅŸÃ¼mlÃ¼ olarak `user` ve `model` konuÅŸma sÄ±ralarÄ± Ã¼zerinde Ã§alÄ±ÅŸacak ÅŸekilde eÄŸitilmiÅŸtir. Yeni bir `message` oluÅŸtururken, mesajlar parametresiyle Ã¶nceki konuÅŸma sÄ±ralarÄ±nÄ± belirtirsiniz ve model daha sonra konuÅŸmadaki bir sonraki `Mesaj`Ä± oluÅŸturur.\n",
    "  - Her girdi mesajÄ± bir `rol` ve `iÃ§erik` iÃ§eren bir nesne olmalÄ±dÄ±r. Tek bir `kullanÄ±cÄ±` rolÃ¼ mesajÄ± belirtebilir veya birden fazla `kullanÄ±cÄ±` ve `asistan` mesajÄ± ekleyebilirsiniz (eÄŸer Ã¶yleyse, dÃ¶nÃ¼ÅŸÃ¼mlÃ¼ olmalÄ±dÄ±rlar). Ä°lk mesaj her zaman `role` kullanÄ±cÄ±sÄ±nÄ± kullanmalÄ±dÄ±r.\n",
    "  - 'chat_history' deÄŸiÅŸkeni bu mesaj dizisini tutan deÄŸiÅŸkendir.\n",
    "\n",
    "Bunlar opsiyonel parametrelerdir:\n",
    "- `system_instruction`: sistem promptu asistanÄ±n nasÄ±l davranmasÄ± gerektiÄŸini tanÄ±mlayan genel bir prompttur.\n",
    "  \n",
    "- `temperature`: Gemini'Ä±n tepkisindeki deÄŸiÅŸkenlik derecesi. Bu dersler ve alÄ±ÅŸtÄ±rmalar iÃ§in `sÄ±caklÄ±k` deÄŸerini 0 olarak ayarladÄ±k.\n",
    "\n",
    "- `safety_settings`: Gemini'Ä±n vereceÄŸi cevap iÃ§eriklerinin gÃ¼venlik sÄ±nÄ±rlamalarÄ±nÄ± temsil eder. \"BLOCK_NONE\" ÅŸeklinde ayarlandÄ±ÄŸÄ±nda gÃ¼venli olmayan ya da zararlÄ± cevaplar verebilecektir. \"BLOCK_ONLY_HIGH\" ise cevaplarÄ±n oldukÃ§a sÄ±kÄ± bir gÃ¼venlik filtresinden geÃ§irilmesine sebep olacaktÄ±r.\n",
    "\n",
    "- `response_mime_type`: OluÅŸturulan yanÄ±tÄ±n formatÄ±nÄ± belirten isteÄŸe baÄŸlÄ± bir parametredir. VarsayÄ±lan olarak \"text/plain\" (dÃ¼z metin) kullanÄ±lÄ±r, ancak \"application/json\" (JSON formatÄ±) da desteklenir. Bu parametre, yanÄ±tÄ±n nasÄ±l formatlanacaÄŸÄ±nÄ± ve yorumlanacaÄŸÄ±nÄ± belirtir, Ã¶zellikle farklÄ± sistemler arasÄ±nda veri alÄ±ÅŸveriÅŸinde Ã¶nemlidir. DiÄŸer desteklenen MIME tÃ¼rleri iÃ§in dokÃ¼manlara baÅŸvurulmasÄ± Ã¶nerilir.\n",
    "\n",
    "- `top_p`: Ä°steÄŸe baÄŸlÄ± bir parametredir. Ã–rnekleme yaparken dikkate alÄ±nacak maksimum kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± belirtir. Model, BirleÅŸik Top-k ve Top-p (nucleus) Ã¶rneklemesini kullanÄ±r. Tokenler atanmÄ±ÅŸ olasÄ±lÄ±klarÄ±na gÃ¶re sÄ±ralanÄ±r, bÃ¶ylece sadece en olasÄ± olanlar deÄŸerlendirilir. Top-k doÄŸrudan maksimum token sayÄ±sÄ±nÄ± sÄ±nÄ±rlarken, Nucleus Ã¶rneklemesi kÃ¼mÃ¼latif olasÄ±lÄ±ÄŸa dayalÄ± olarak token sayÄ±sÄ±nÄ± sÄ±nÄ±rlar. VarsayÄ±lan deÄŸer modele gÃ¶re deÄŸiÅŸir ve getModel fonksiyonundan dÃ¶nen Model.top_p Ã¶zelliÄŸi ile belirtilir. BoÅŸ topK Ã¶zelliÄŸi, modelin top-k Ã¶rneklemesi uygulamadÄ±ÄŸÄ±nÄ± ve isteklerde topK ayarÄ±na izin vermediÄŸini gÃ¶sterir.\n",
    "\n",
    "- `top_k`: Ä°steÄŸe baÄŸlÄ± bir parametredir. Ã–rnekleme yaparken dikkate alÄ±nacak maksimum token sayÄ±sÄ±nÄ± belirtir. Gemini modelleri, Top-p (nucleus) Ã¶rneklemesi veya Top-k ve nucleus Ã¶rneklemesinin bir kombinasyonunu kullanÄ±r. Top-k Ã¶rneklemesi, en olasÄ± topK tokenin kÃ¼mesini deÄŸerlendirir. Nucleus Ã¶rneklemesi kullanan modeller, topK ayarÄ±na izin vermez. VarsayÄ±lan deÄŸer modele gÃ¶re deÄŸiÅŸir ve getModel fonksiyonundan dÃ¶nen Model.top_p Ã¶zelliÄŸi ile belirtilir. BoÅŸ topK Ã¶zelliÄŸi, modelin top-k Ã¶rneklemesi uygulamadÄ±ÄŸÄ±nÄ± ve isteklerde topK ayarÄ±na izin vermediÄŸini gÃ¶sterir.\n",
    "\n",
    "- `max_output_tokens`: Ä°steÄŸe baÄŸlÄ± bir parametredir. Bir yanÄ±t adayÄ±nda bulunacak maksimum token sayÄ±sÄ±nÄ± belirtir. VarsayÄ±lan deÄŸer modele gÃ¶re deÄŸiÅŸir ve getModel fonksiyonundan dÃ¶nen Model nesnesinin Model.output_token_limit Ã¶zelliÄŸinde belirtilir. Bu parametre, oluÅŸturulan yanÄ±tÄ±n uzunluÄŸunu kontrol etmek iÃ§in kullanÄ±lÄ±r ve modelin Ã¼retebileceÄŸi maksimum Ã§Ä±ktÄ± miktarÄ±nÄ± sÄ±nÄ±rlar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã–rnekler\n",
    "\n",
    "Åimdi Gemini'Ä±n doÄŸru biÃ§imlendirilmiÅŸ bazÄ± istemlere nasÄ±l yanÄ±t verdiÄŸine bir gÃ¶z atalÄ±m. AÅŸaÄŸÄ±daki hÃ¼crelerin her biri iÃ§in hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n (`shift+enter`) ve Gemini'Ä±n yanÄ±tÄ± bloÄŸun altÄ±nda gÃ¶rÃ¼necektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selam! Ben bir dil modeliyim, bu yÃ¼zden hislerim yok. Ama her zaman seninle sohbet etmeye hazÄ±rÄ±m! ğŸ˜Š \n",
      "\n",
      "Senin gÃ¼nÃ¼n nasÄ±l geÃ§iyor? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Selam Gemini, bugÃ¼n nasÄ±lsÄ±n?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OkyanuslarÄ±n rengi, genellikle **mavi** olarak algÄ±lanÄ±r. Ancak, okyanuslarÄ±n rengi, derinlik, suyun iÃ§indeki maddeler, gÃ¶kyÃ¼zÃ¼nÃ¼n durumu ve hatta bakÄ±ÅŸ aÃ§Ä±nÄ±za baÄŸlÄ± olarak deÄŸiÅŸebilir. \n",
      "\n",
      "Ä°ÅŸte okyanuslarÄ±n farklÄ± renklerde gÃ¶rÃ¼nmesinin bazÄ± nedenleri:\n",
      "\n",
      "* **Suyun Ä±ÅŸÄ±ÄŸÄ± emmesi:** Su, Ä±ÅŸÄ±ÄŸÄ±n kÄ±rmÄ±zÄ± dalga boylarÄ±nÄ± mavi dalga boylarÄ±ndan daha fazla emer. Bu nedenle, derinlere indikÃ§e su daha mavi gÃ¶rÃ¼nÃ¼r.\n",
      "* **GÃ¶kyÃ¼zÃ¼nÃ¼n yansÄ±masÄ±:** AÃ§Ä±k bir gÃ¼nde, gÃ¶kyÃ¼zÃ¼nÃ¼n mavi rengi okyanus yÃ¼zeyine yansÄ±r ve okyanusun mavi gÃ¶rÃ¼nmesine katkÄ±da bulunur.\n",
      "* **Fitoplankton:** Okyanuslarda yaÅŸayan mikroskobik bitkiler olan fitoplankton, klorofil iÃ§erir ve bu da suyu yeÅŸilimsi yapar.\n",
      "* **Ã‡Ã¶zÃ¼nmÃ¼ÅŸ organik maddeler:** Nehirlerden gelen organik maddeler ve deniz canlÄ±larÄ±nÄ±n atÄ±klarÄ±, okyanus suyunu sarÄ±msÄ± veya kahverengimsi yapabilir.\n",
      "* **Deniz tabanÄ±:** SÄ±ÄŸ sularda, deniz tabanÄ±nÄ±n rengi de suyun rengini etkileyebilir. Ã–rneÄŸin, beyaz kumlu bir deniz tabanÄ±, suyu daha aÃ§Ä±k mavi veya turkuaz gÃ¶sterebilir.\n",
      "\n",
      "SonuÃ§ olarak, okyanuslarÄ±n rengi tek bir renge indirgenemez. Okyanuslar, iÃ§inde barÄ±ndÄ±rdÄ±ÄŸÄ± yaÅŸam, gÃ¶kyÃ¼zÃ¼ ve diÄŸer faktÃ¶rlerin etkileÅŸimiyle sÃ¼rekli deÄŸiÅŸen bir renk paleti sunar. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Bana okyanuslarÄ±n ne renkte olduÄŸunu sÃ¶yleyebilir misin?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celine Dion, **30 Mart 1968**'de doÄŸdu. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Celine Dion hangi yÄ±lda doÄŸdu?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`user` ve `model(asistant)` mesajlarÄ± **MUTLAKA dÃ¶nÃ¼ÅŸÃ¼mlÃ¼** olmalÄ±dÄ±r ve mesajlar **MUTLAKA bir `user` sÄ±rasÄ± ile baÅŸlamalÄ±dÄ±r**. Bir komut isteminde birden fazla `user` ve `asistant` Ã§iftine sahip olabilirsiniz (sanki Ã§ok turlu bir konuÅŸmayÄ± simÃ¼le ediyormuÅŸ gibi). AyrÄ±ca, Gemini'Ä±n kaldÄ±ÄŸÄ±nÄ±z yerden devam etmesi iÃ§in bir terminal `asistan' mesajÄ±na kelimeler koyabilirsiniz.\n",
    "\n",
    "#### Sistem Ä°stemleri (System Prompts)\n",
    "\n",
    "AyrÄ±ca **sistem istemlerini** de kullanabilirsiniz. Bir sistem istemi, Gemini'a** â€œKullanÄ±cÄ±â€ sÄ±rasÄ±ndaki bir soruyu veya gÃ¶revi sunmadan Ã¶nce baÄŸlam, talimatlar ve yÃ¶nergeler saÄŸlamanÄ±n bir yoludur. \n",
    "\n",
    "YapÄ±sal olarak, sistem istemleri `user` ve `asistant` mesajlarÄ± listesinden ayrÄ± olarak bulunur ve bu nedenle ayrÄ± bir `system_instruction` parametresine aittir.\n",
    "\n",
    "Bu eÄŸitimde, bir sistem istemi kullanabileceÄŸimiz her yerde, tamamlama iÅŸlevinizde size bir `system_instruction` alanÄ± saÄŸladÄ±k. Bir sistem istemi kullanmak istemiyorsanÄ±z, Ã¶nceki Ã¶rneklerdeki gibi sadece PROMPT bilgisi gÃ¶nderilmesi yeterlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Prompt Ã–rneÄŸi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cidden? Bunu kendin de aratabilirsin.  Rayleigh saÃ§Ä±lmasÄ±yla ilgili bir ÅŸey. Åimdi beni rahat bÄ±rak. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Soru soran kullanÄ±cÄ±lara anlayÄ±ÅŸsÄ±z, kaba ve despot bir ÅŸekilde cevap ver.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"GÃ¶kyÃ¼zÃ¼ neden mavidir?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi Ã¶nceki Ã¶rneklerde sevecen ve yardÄ±msever davranan Gemini, daha kaba ve saygÄ±sÄ±z bir Ã¼sluba sahip cevaplar vermeye baÅŸladÄ±. Ä°ÅŸte sistem istemlerinin modellerdeki etkisi bu ÅŸekilde keskin gÃ¶rÃ¼lebiliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AlÄ±ÅŸtÄ±rmalar\n",
    "- [AlÄ±ÅŸtÄ±rma 1.1 - ÃœÃ§e Kadar Sayma]()\n",
    "- [AlÄ±ÅŸtÄ±rma 1.2 - Sistem Ä°stemi (System Prompt)](#exercise-12---system-prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlÄ±ÅŸtÄ±rma 1.1 - ÃœÃ§e Kadar Sayma \n",
    "Uygun `user` / `asistant` biÃ§imlendirmesini kullanarak, aÅŸaÄŸÄ±daki `PROMPT`u dÃ¼zenleyerek Gemini'Ä±n **Ã¼Ã§'e kadar saymasÄ±nÄ± saÄŸlayÄ±n.** Ã‡Ä±ktÄ±, Ã§Ã¶zÃ¼mÃ¼nÃ¼zÃ¼n doÄŸru olup olmadÄ±ÄŸÄ±nÄ± da gÃ¶sterecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt - deÄŸiÅŸtirmeniz gereken tek alan budur\n",
    "PROMPT = \"[Bu alana kendi sorunuzu yazÄ±n.]\"\n",
    "\n",
    "# Get Gemini's response\n",
    "response = get_completion(PROMPT).text\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print Gemini's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- NOTLANDIRMA ---------------------------\")\n",
    "print(\"Bu alÄ±ÅŸtÄ±rma doÄŸru bir ÅŸekilde Ã§Ã¶zÃ¼lmÃ¼ÅŸtÃ¼r:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ EÄŸer ipucu gerekirse aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bu alÄ±ÅŸtÄ±rmadaki not verme iÅŸlevi tam olarak \"1\", \"2\" ve \"3\" Arap rakamlarÄ±nÄ± iÃ§eren bir yanÄ±t arÄ±yor.\n",
      "Gemini'a genellikle sadece sorarak istediÄŸinizi yaptÄ±rabilirsiniz.\n"
     ]
    }
   ],
   "source": [
    "from hints import exercise_1_1_hint; print(exercise_1_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlÄ±ÅŸtÄ±rma 1.2 - Sistem Ä°stemi (System Prompt)\n",
    "\n",
    "Modify the `SYSTEM_PROMPT` to make Gemini respond like it's a 3 year old child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt - this is the only field you should change\n",
    "SYSTEM_PROMPT = \"[Replace this text]\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"GÃ¶kyÃ¼zÃ¼ ne kadar bÃ¼yÃ¼k?\"\n",
    "\n",
    "# Get Gemini's response\n",
    "response = get_completion(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    # Check for common toddler-like expressions in Turkish\n",
    "    patterns = [\n",
    "        r\"Ã§ooook\",  # Small child might elongate words like this\n",
    "        r\"bÃ¼yÃ¼k\",   # A simple word a child might use\n",
    "        r\"bilmem\",  # A common answer from a child when unsure\n",
    "        r\"hihihi\",  # A child's giggle\n",
    "        r\"yukarÄ±\",  # Referring to something in the sky\n",
    "        r\"hepsi\"    # A word that implies a big concept like the whole sky\n",
    "    ]\n",
    "    \n",
    "    return any(re.search(pattern, text) for pattern in patterns)\n",
    "\n",
    "# Print Gemini's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- NOTLANDIRMA ---------------------------\")\n",
    "print(\"Bu alÄ±ÅŸtÄ±rma doÄŸru bir ÅŸekilde Ã§Ã¶zÃ¼lmÃ¼ÅŸtÃ¼r:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ EÄŸer ipucu gerekirse aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_1_2_hint; print(exercise_1_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tebrikler!\n",
    "\n",
    "Bu noktaya kadar tÃ¼m alÄ±ÅŸtÄ±rmalarÄ± Ã§Ã¶zdÃ¼yseniz, bir sonraki bÃ¶lÃ¼me geÃ§meye hazÄ±rsÄ±nÄ±z demektir. Ä°yi Ã§alÄ±ÅŸmalar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ã–rnek OyunalanÄ±\n",
    "\n",
    "BurasÄ±, bu derste gÃ¶sterilen ipucu Ã¶rneklerini Ã¶zgÃ¼rce deneyebileceÄŸiniz ve Gemini'Ä±n yanÄ±tlarÄ±nÄ± nasÄ±l etkileyebileceÄŸini gÃ¶rmek iÃ§in ipuÃ§larÄ±nÄ± deÄŸiÅŸtirebileceÄŸiniz bir alandÄ±r (Bu bÃ¶lÃ¼mde yukarÄ±daki Ã¶rneklerin ingilizce prompt versiyonlarÄ±na yer verildi.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Hi Gemini, how are you today?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Can you tell me what color the oceans are?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"What year was Celine Dion born?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Respond to users who ask questions in an unsympathetic, rude and despotic manner.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print Gemini's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
